%
\begin{Frame}{SWParallelismLevel MPI}
  \begin{columns}[t]
    \begin{column}{\HW} % Colonne gauche
      \begin{block}{Message Passing Interface}
        \begin{itemize}
        \item \texttt{mpi\_send()}, \texttt{mpi\_receive()}, 
        \item
          \WP{Message\_Passing\_Interface}
        \item Share data by sending message with or without shared memory
        \end{itemize}
      \end{block} 
      \begin{block}{Usage Scenarios}
        \begin{itemize}
        \item Single node, shared memory : multiple processes : testing
        \item Multiple node, distributed memory : 1 process / node : scaling
        \end{itemize}
      \end{block} 
    \end{column}
    
    \begin{column}{\HW} % Colonne droite
      \begin{alertblock}{Comments}
        \begin{itemize}
        \item Normalized: No (de facto)
        \item Portable : Yes
        \item Scalable : Yes, why ?
        \item Warning : data locks 
        \end{itemize}

        % \Image{CUDA/Name.png}
      \end{alertblock}   
        Open source implementation \href{https://www.mpich.org/}{https://www.mpich.org/}
    \end{column}
  \end{columns}  
\end{Frame}

%% Local Variables:
%% mode: latex
%% coding: utf-8
%% ispell-dictionary: "american"
%% TeX-master: "../../main.tex"
%% End:

